{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: Harshal Bhasgauri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qHV0JTKZJIk"
      },
      "source": [
        "1.importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G-3m5jjgWr48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPqe-XhTZNfI"
      },
      "source": [
        "2.initializing the environment and setting is_slipper= False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE4OenpAWytD",
        "outputId": "2d1bd580-add9-4089-f526-611d6b5a59c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W8qeoEdZWrp"
      },
      "source": [
        "size of action space and state spacw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4H1etrOUVKk",
        "outputId": "74379ffc-aa19-4d62-9ce6-6e71c54a0ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space size: 4\n",
            "State space size: 16\n"
          ]
        }
      ],
      "source": [
        "action_space_size = env.action_space.n\n",
        "print(\"Action space size:\", action_space_size)\n",
        "\n",
        "state_space_size = env.observation_space.n\n",
        "print(\"State space size:\", state_space_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnT0lf0wZm5A"
      },
      "source": [
        "3.Perform policy evaluation iterations until the smallest change is less\n",
        "than smallest_change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0y0ayonCW4GD"
      },
      "outputs": [],
      "source": [
        "gamma = 0.9\n",
        "smallest_change = 1e-6\n",
        "\n",
        "V = np.zeros(env.observation_space.n)# initialize the value function array\n",
        "\n",
        "def policy_evaluation(policy, V):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.observation_space.n):\n",
        "            v = V[s]\n",
        "            # using the bellman equation\n",
        "            V[s] = sum([p*(r+gamma*V[s_]) for p, s_, r, _ in env.P[s][policy[s]]])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < smallest_change:\n",
        "            break\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqD9tD9iemSa"
      },
      "source": [
        "here I initialized the value function array V to zeros, and define the policy_evaluation() function, which updates the value function using the Bellman equation until the smallest change is less than the specified threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg6i4RPFaXUy"
      },
      "source": [
        "performing policy evaluation iterations until convergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WWcW656aVit",
        "outputId": "82a872ae-470d-46b6-e2ec-71b59a4761e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "value function: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "policy_var = np.zeros(env.observation_space.n, dtype=int)\n",
        "V = policy_evaluation(policy_var, V)\n",
        "print(\"value function:\", V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsDWFgItblK7"
      },
      "source": [
        "4.perform policy improvement using the Bellman optimality equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_9DjQIUzbTXS"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(policy, V):\n",
        "    policy_stable = True\n",
        "    for s in range(env.observation_space.n):\n",
        "        old_action = policy[s]\n",
        "        \n",
        "        action_values = [sum([p*(r+gamma*V[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)]\n",
        "        # best_action is the action wtih the highest val\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy[s] = best_action\n",
        "        \n",
        "        if old_action != best_action:\n",
        "            policy_stable = False\n",
        "    return policy, policy_stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caHG3CP6fH8y"
      },
      "source": [
        "i perrformed policy iteration until convergence by repeatedly calling policy_evaluation() and policy_improvement() until the policy becomes stable, i.e., the policy does not change after a policy improvement step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjMPzuqDdftE"
      },
      "source": [
        "5.Find the most optimal policy for the FrozenLake-v1 environment using policy iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dhhMg-WeW95W"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    V = policy_evaluation(policy_var, V)\n",
        "    policy, policy_stable = policy_improvement(policy_var, V)\n",
        "    if policy_stable:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ5fqSfkXCbL",
        "outputId": "d433a6bb-8f2e-4df6-cf13-8b1a4b7eb685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"final policy:\", policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZAjmKLcGCB"
      },
      "source": [
        "7.take steps through the FrozenLake-v1 environment randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkbyiwNacwL2",
        "outputId": "ec7cc10e-d114-44ba-88aa-597c9951fc47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step: 1 Action: 1 State: 4 Reward: 0.0 Done: False\n",
            "Step: 2 Action: 0 State: 4 Reward: 0.0 Done: False\n",
            "Step: 3 Action: 1 State: 8 Reward: 0.0 Done: False\n",
            "Step: 4 Action: 1 State: 12 Reward: 0.0 Done: True\n",
            "Step: 5 Action: 2 State: 1 Reward: 0.0 Done: False\n",
            "Step: 6 Action: 3 State: 1 Reward: 0.0 Done: False\n",
            "Step: 7 Action: 0 State: 0 Reward: 0.0 Done: False\n",
            "Step: 8 Action: 3 State: 0 Reward: 0.0 Done: False\n",
            "Step: 9 Action: 0 State: 0 Reward: 0.0 Done: False\n",
            "Step: 10 Action: 2 State: 1 Reward: 0.0 Done: False\n",
            "Step: 11 Action: 3 State: 1 Reward: 0.0 Done: False\n",
            "Step: 12 Action: 2 State: 2 Reward: 0.0 Done: False\n",
            "Step: 13 Action: 1 State: 6 Reward: 0.0 Done: False\n",
            "Step: 14 Action: 2 State: 7 Reward: 0.0 Done: True\n",
            "Step: 15 Action: 1 State: 4 Reward: 0.0 Done: False\n",
            "Step: 16 Action: 0 State: 4 Reward: 0.0 Done: False\n"
          ]
        }
      ],
      "source": [
        "num_steps = 16\n",
        "for i in range(num_steps):\n",
        "    action = env.action_space.sample() \n",
        "    state, reward, done, info = env.step(action) \n",
        "    print(\"Step:\", i+1, \"Action:\", action, \"State:\", state, \"Reward:\", reward, \"Done:\", done)\n",
        "    if done: \n",
        "        state = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2nFG1o2fz4M"
      },
      "source": [
        "I have set the number of steps to take through the environment using num_steps= 16.\n",
        "\n",
        "taking num_steps random steps through the environment using a for loop that iterates from 0 to (num_steps)-1. In each iteration, we select a random action using env.action_space.sample(), and take a step in the environment using env.step(action). We print the action, state, reward, and done flag returned by env.step(). If the episode has terminated (i.e., done=True), we reset the environment using env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJOgrDh1c5nW"
      },
      "source": [
        "6.Perform a test pass on the FrozenLake-v1 environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7GDb4wrXHV4",
        "outputId": "ddf50605-197e-4f1c-b895-81fe34964f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode no: 1  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 2  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 3  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 4  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 5  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 6  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 7  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 8  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 9  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 10  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 11  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 12  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 13  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 14  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 15  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 16  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 17  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 18  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 19  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 20  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 21  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 22  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 23  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 24  , total reward: 1.0 \n",
            " done \n",
            "\n",
            "episode no: 25  , total reward: 1.0 \n",
            " done \n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_episodes = 25    \n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "    print(\"episode no:\", episode+1, \" , total reward:\", total_reward, '\\n' , \"done\", '\\n')\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfTSWplAgSV8"
      },
      "source": [
        "^performing a test pass using the optimal policy by running num_episodes episodes of the environment. For each episode, we reset the environment using env.reset(), and then repeatedly select actions according to the optimal policy using policy[state] until the episode terminates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21IaqyVIdm2g"
      },
      "source": [
        "8.Perform value iteration to find the most optimal policy for the\n",
        "FrozenLake-v1 environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC-kX0GIY_8N",
        "outputId": "1d49e66d-5296-4acf-8116-46b7a11bf83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal policy: [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-16-80b10774f617>:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  policy = np.zeros(num_states, dtype=np.int)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "discount_factor = 0.99\n",
        "smallest_change = 1e-6\n",
        "\n",
        "V = np.zeros(num_states)\n",
        "\n",
        "while True:\n",
        "    delta = 0\n",
        "\n",
        "    for s in range(num_states):\n",
        "        q_values = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward, done in env.P[s][a]:\n",
        "                q_values[a] += prob * (reward + discount_factor * V[next_state])\n",
        "        best_action_value = np.max(q_values)\n",
        "\n",
        "        change = np.abs(best_action_value - V[s])\n",
        "        delta = max(delta, change)\n",
        "        V[s] = best_action_value\n",
        "\n",
        "    if delta < smallest_change:\n",
        "        break\n",
        "\n",
        "policy = np.zeros(num_states, dtype=np.int)\n",
        "for s in range(num_states):\n",
        "    q_values = np.zeros(num_actions)\n",
        "    for a in range(num_actions):\n",
        "        for prob, next_state, reward, done in env.P[s][a]:\n",
        "            q_values[a] += prob * (reward + discount_factor * V[next_state])\n",
        "    best_action = np.argmax(q_values)\n",
        "    policy[s] = best_action\n",
        "\n",
        "print(\"Optimal policy:\", policy)\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
